{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH-sF6hRXeBZ"
      },
      "source": [
        "<font color=\"#4b76b7\">To start practicing, you will need to make a copy of it. Go to File > Save a Copy in Drive. You can then use the new copy that will appear in the new tab.</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dC3nTnLyZNnv"
      },
      "source": [
        "# AfterWork Data Science: Getting Started with NLP Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enxDwtXPZTpT"
      },
      "source": [
        "### Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "dz7plWFhaQ9R"
      },
      "outputs": [],
      "source": [
        "# Importing the required libraries\n",
        "# ---\n",
        "# \n",
        "import pandas as pd # library for data manipulation\n",
        "import numpy as np  # librariy for scientific computations\n",
        "import re           # regex library to perform text preprocessing\n",
        "import string       # library to work with strings\n",
        "import nltk         # library for natural language processing\n",
        "import scipy        # scientific conputing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85tygFqJZ0Xw"
      },
      "source": [
        "### 1. Importing our Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "r_d2EpfjafP3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>0</th>\n",
              "      <th>1467810369</th>\n",
              "      <th>Mon Apr 06 22:19:45 PDT 2009</th>\n",
              "      <th>NO_QUERY</th>\n",
              "      <th>_TheSpecialOne_</th>\n",
              "      <th>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>346508</td>\n",
              "      <td>0</td>\n",
              "      <td>2016177685</td>\n",
              "      <td>Wed Jun 03 06:18:50 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>UriGrey</td>\n",
              "      <td>Obama forges his Muslim alliance against the c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>883537</td>\n",
              "      <td>4</td>\n",
              "      <td>1686152287</td>\n",
              "      <td>Sun May 03 04:02:08 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>MariesolW</td>\n",
              "      <td>Had the most spectacular prom ever  but now my...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>764173</td>\n",
              "      <td>0</td>\n",
              "      <td>2298725623</td>\n",
              "      <td>Tue Jun 23 12:02:12 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>ColleenBurns</td>\n",
              "      <td>I am overwhelmed today  taking a moment to eat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>638701</td>\n",
              "      <td>0</td>\n",
              "      <td>2234530495</td>\n",
              "      <td>Thu Jun 18 23:13:54 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>queenarchy</td>\n",
              "      <td>@lindork Tres sad. I was totally a Max fan.  #...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>664821</td>\n",
              "      <td>0</td>\n",
              "      <td>2244623416</td>\n",
              "      <td>Fri Jun 19 14:59:46 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>reinventingjess</td>\n",
              "      <td>Crap, I was counting down the hours until my d...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  \\\n",
              "0      346508  0  2016177685  Wed Jun 03 06:18:50 PDT 2009  NO_QUERY   \n",
              "1      883537  4  1686152287  Sun May 03 04:02:08 PDT 2009  NO_QUERY   \n",
              "2      764173  0  2298725623  Tue Jun 23 12:02:12 PDT 2009  NO_QUERY   \n",
              "3      638701  0  2234530495  Thu Jun 18 23:13:54 PDT 2009  NO_QUERY   \n",
              "4      664821  0  2244623416  Fri Jun 19 14:59:46 PDT 2009  NO_QUERY   \n",
              "\n",
              "   _TheSpecialOne_  \\\n",
              "0          UriGrey   \n",
              "1        MariesolW   \n",
              "2     ColleenBurns   \n",
              "3       queenarchy   \n",
              "4  reinventingjess   \n",
              "\n",
              "  @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
              "0  Obama forges his Muslim alliance against the c...                                                                   \n",
              "1  Had the most spectacular prom ever  but now my...                                                                   \n",
              "2  I am overwhelmed today  taking a moment to eat...                                                                   \n",
              "3  @lindork Tres sad. I was totally a Max fan.  #...                                                                   \n",
              "4  Crap, I was counting down the hours until my d...                                                                   "
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Question: Given a new tweets, create a sentiment analysis model that will \n",
        "# predict whether a tweet will contain positive or negative sentiment.\n",
        "# ---\n",
        "# Dataset url = https://bit.ly/31kqByD \n",
        "# ---\n",
        "#\n",
        "\n",
        "# see entire column content in the dataframe\n",
        "pd.set_option('display.max_columns', None)  \n",
        "\n",
        "df = pd.read_csv('https://bit.ly/31kqByD', encoding='latin-1')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96Uz3PxJZ6E7"
      },
      "source": [
        "### 2. Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Tv_yGMvFbZtL"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10000, 7)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We can determine the size of our dataset\n",
        "# ---\n",
        "#\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDtEQCn6oAwJ"
      },
      "source": [
        "Seems this dataset will need some data cleaning i.e. columns. We also don't need some columns to perform create our model. We will drop those columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvsnLPXTZ8P0"
      },
      "source": [
        "### 3. Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlHYpKxfHRJ4"
      },
      "source": [
        "#### Basic Data Cleaning Techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "3hpqmVDWbfcM"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "      <th>t_id</th>\n",
              "      <th>created_at</th>\n",
              "      <th>query</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3959</th>\n",
              "      <td>1276060</td>\n",
              "      <td>4</td>\n",
              "      <td>2001165871</td>\n",
              "      <td>Tue Jun 02 00:02:11 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>sabrinasg</td>\n",
              "      <td>@m4s Looking forward to it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8777</th>\n",
              "      <td>480659</td>\n",
              "      <td>0</td>\n",
              "      <td>2179337746</td>\n",
              "      <td>Mon Jun 15 08:58:14 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>skpargania</td>\n",
              "      <td>I am very tired</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7327</th>\n",
              "      <td>623899</td>\n",
              "      <td>0</td>\n",
              "      <td>2229720785</td>\n",
              "      <td>Thu Jun 18 16:06:46 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>supergeeker</td>\n",
              "      <td>@ackstay yeah, um, she left like, insides &amp;amp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8920</th>\n",
              "      <td>556123</td>\n",
              "      <td>0</td>\n",
              "      <td>2204214768</td>\n",
              "      <td>Wed Jun 17 01:37:29 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>swellvintage</td>\n",
              "      <td>@jamiesmart Still no internet  soz. I WILL be ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6278</th>\n",
              "      <td>586708</td>\n",
              "      <td>0</td>\n",
              "      <td>2216055355</td>\n",
              "      <td>Wed Jun 17 18:58:53 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>JLSapphire914</td>\n",
              "      <td>Somebody send me a picture text message cuz I ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6759</th>\n",
              "      <td>838099</td>\n",
              "      <td>4</td>\n",
              "      <td>1558975990</td>\n",
              "      <td>Sun Apr 19 09:55:23 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>H4L3Yx</td>\n",
              "      <td>jus woke upp eatin breakfast at noon! i love w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8466</th>\n",
              "      <td>313442</td>\n",
              "      <td>0</td>\n",
              "      <td>2001752926</td>\n",
              "      <td>Tue Jun 02 01:57:26 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>weezii_d</td>\n",
              "      <td>yeah assignments</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>869</th>\n",
              "      <td>1130980</td>\n",
              "      <td>4</td>\n",
              "      <td>1975786067</td>\n",
              "      <td>Sat May 30 15:54:02 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mmystifier</td>\n",
              "      <td>Now back at home....poker stars on Tv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4012</th>\n",
              "      <td>410949</td>\n",
              "      <td>0</td>\n",
              "      <td>2059992814</td>\n",
              "      <td>Sat Jun 06 18:02:56 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>animarieee</td>\n",
              "      <td>Where is everyone?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2911</th>\n",
              "      <td>404343</td>\n",
              "      <td>0</td>\n",
              "      <td>2058399791</td>\n",
              "      <td>Sat Jun 06 14:51:55 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mayank25may</td>\n",
              "      <td>@Krishna_Teja dude steve is not giving the key...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           id  target        t_id                    created_at     query  \\\n",
              "3959  1276060       4  2001165871  Tue Jun 02 00:02:11 PDT 2009  NO_QUERY   \n",
              "8777   480659       0  2179337746  Mon Jun 15 08:58:14 PDT 2009  NO_QUERY   \n",
              "7327   623899       0  2229720785  Thu Jun 18 16:06:46 PDT 2009  NO_QUERY   \n",
              "8920   556123       0  2204214768  Wed Jun 17 01:37:29 PDT 2009  NO_QUERY   \n",
              "6278   586708       0  2216055355  Wed Jun 17 18:58:53 PDT 2009  NO_QUERY   \n",
              "6759   838099       4  1558975990  Sun Apr 19 09:55:23 PDT 2009  NO_QUERY   \n",
              "8466   313442       0  2001752926  Tue Jun 02 01:57:26 PDT 2009  NO_QUERY   \n",
              "869   1130980       4  1975786067  Sat May 30 15:54:02 PDT 2009  NO_QUERY   \n",
              "4012   410949       0  2059992814  Sat Jun 06 18:02:56 PDT 2009  NO_QUERY   \n",
              "2911   404343       0  2058399791  Sat Jun 06 14:51:55 PDT 2009  NO_QUERY   \n",
              "\n",
              "               user                                               text  \n",
              "3959      sabrinasg                        @m4s Looking forward to it   \n",
              "8777     skpargania                                   I am very tired   \n",
              "7327    supergeeker  @ackstay yeah, um, she left like, insides &amp...  \n",
              "8920   swellvintage  @jamiesmart Still no internet  soz. I WILL be ...  \n",
              "6278  JLSapphire914  Somebody send me a picture text message cuz I ...  \n",
              "6759         H4L3Yx  jus woke upp eatin breakfast at noon! i love w...  \n",
              "8466       weezii_d                                  yeah assignments   \n",
              "869      mmystifier             Now back at home....poker stars on Tv   \n",
              "4012     animarieee                                Where is everyone?   \n",
              "2911    mayank25may  @Krishna_Teja dude steve is not giving the key...  "
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We rename the columns for ease of referencing our columns later on\n",
        "# ---\n",
        "#\n",
        "df.columns = ['id', 'target', 't_id', 'created_at', 'query', 'user', 'text']\n",
        "df.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "5HSGdDX3bjUB"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8796</th>\n",
              "      <td>4</td>\n",
              "      <td>told my mum my maths result. she said she'll s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2597</th>\n",
              "      <td>4</td>\n",
              "      <td>it's my day off.. finally!! a nice rainy day 4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9056</th>\n",
              "      <td>0</td>\n",
              "      <td>@Kristie999 Thanks girl i did over just a litt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>573</th>\n",
              "      <td>0</td>\n",
              "      <td>I have no money to actually buy playboy, other...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5183</th>\n",
              "      <td>4</td>\n",
              "      <td>at home  thank god.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>445</th>\n",
              "      <td>0</td>\n",
              "      <td>sry guys some probs wid yahoo messenger... can...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5430</th>\n",
              "      <td>0</td>\n",
              "      <td>scared of tornado warnings</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2577</th>\n",
              "      <td>4</td>\n",
              "      <td>@ilex_ Hey you   Seeing clearly yet? LOL  Nah,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2566</th>\n",
              "      <td>0</td>\n",
              "      <td>Updated my blog with Good Bye Gary aka the auc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8434</th>\n",
              "      <td>0</td>\n",
              "      <td>Uhhh I have a fever now, I can't stop sweating...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      target                                               text\n",
              "8796       4  told my mum my maths result. she said she'll s...\n",
              "2597       4  it's my day off.. finally!! a nice rainy day 4...\n",
              "9056       0  @Kristie999 Thanks girl i did over just a litt...\n",
              "573        0  I have no money to actually buy playboy, other...\n",
              "5183       4                                at home  thank god.\n",
              "445        0  sry guys some probs wid yahoo messenger... can...\n",
              "5430       0                        scared of tornado warnings \n",
              "2577       4  @ilex_ Hey you   Seeing clearly yet? LOL  Nah,...\n",
              "2566       0  Updated my blog with Good Bye Gary aka the auc...\n",
              "8434       0  Uhhh I have a fever now, I can't stop sweating..."
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We retain the relevant columns by dropping the columns we don't need \n",
        "# for creating a sentiment analysis model. \n",
        "# ---\n",
        "#\n",
        "df = df.drop(['id', 't_id', 'created_at', 'query', 'user'], axis = 1)\n",
        "df.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "EgnEEnresgJl"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    5067\n",
              "4    4933\n",
              "Name: target, dtype: int64"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Understanding the distribution of target\n",
        "# ---\n",
        "#\n",
        "df.target.value_counts() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "a8riNiGfupWL"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "target     int64\n",
              "text      object\n",
              "dtype: object"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's determine whether our columns have the right data types\n",
        "# ---\n",
        "#\n",
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ns54XoFrVcLp"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 4])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# What values are in our target variable?\n",
        "# ---\n",
        "#\n",
        "df.target.unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmYkmFOBVtMC"
      },
      "source": [
        "These are the two classes to which each document (text) belongs. The target value 0 means a text with a negative sentiment, while that of 4 means a text with a positive sentiment. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "-6JoJc4Nvz1S"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "target    0\n",
              "text      0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's check for missing values \n",
        "# ---\n",
        "# \n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxpcyoUfoy9s"
      },
      "source": [
        "We don't have any missing values, so we are good to go."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BdB9m4_yK-1"
      },
      "source": [
        "#### Text Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "S8BW53FUm41_"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Obama forges his Muslim alliance against the c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Had the most spectacular prom ever  but now my...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I am overwhelmed today  taking a moment to eat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@lindork Tres sad. I was totally a Max fan.  #...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Crap, I was counting down the hours until my d...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0  Obama forges his Muslim alliance against the c...\n",
              "1  Had the most spectacular prom ever  but now my...\n",
              "2  I am overwhelmed today  taking a moment to eat...\n",
              "3  @lindork Tres sad. I was totally a Max fan.  #...\n",
              "4  Crap, I was counting down the hours until my d..."
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Text Cleaning: Removing all urls/links\n",
        "# ---\n",
        "# \n",
        "df['text'] =  df['text'].apply(lambda x: re.sub(r'http\\S+|www\\S+|https\\S+','', str(x)))\n",
        "df[['text']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "UR9gL4v8m9mV"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Obama forges his Muslim alliance against the c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>Had the most spectacular prom ever  but now my...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>I am overwhelmed today  taking a moment to eat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>lindork Tres sad. I was totally a Max fan.  #...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Crap, I was counting down the hours until my d...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   target                                               text\n",
              "0       0  Obama forges his Muslim alliance against the c...\n",
              "1       4  Had the most spectacular prom ever  but now my...\n",
              "2       0  I am overwhelmed today  taking a moment to eat...\n",
              "3       0   lindork Tres sad. I was totally a Max fan.  #...\n",
              "4       0  Crap, I was counting down the hours until my d..."
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Text Cleaning: Removing @ and # characters or replace them with space\n",
        "# ---\n",
        "# Remove @\n",
        "\n",
        "# df['no_of_ampersats'] = df.text.apply(lambda x: len([x for x in x.split() if x.startswith('@')]))\n",
        "# df[['text','no_of_ampersats']].head(10)\n",
        "\n",
        "df['text'] = df.text.str.replace('@',' ')\n",
        "df.head(5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Obama forges his Muslim alliance against the c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>Had the most spectacular prom ever  but now my...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>I am overwhelmed today  taking a moment to eat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>lindork Tres sad. I was totally a Max fan.   ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Crap, I was counting down the hours until my d...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   target                                               text\n",
              "0       0  Obama forges his Muslim alliance against the c...\n",
              "1       4  Had the most spectacular prom ever  but now my...\n",
              "2       0  I am overwhelmed today  taking a moment to eat...\n",
              "3       0   lindork Tres sad. I was totally a Max fan.   ...\n",
              "4       0  Crap, I was counting down the hours until my d..."
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Remove #\n",
        "\n",
        "df['text'] = df.text.str.replace('#',' ')\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "3cTS7iaXdAQz"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>no_of_uppercase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Obama forges his Muslim alliance against the c...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Had the most spectacular prom ever  but now my...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I am overwhelmed today  taking a moment to eat...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>lindork Tres sad. I was totally a Max fan.   ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Crap, I was counting down the hours until my d...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>DCBTV  DCBTV I had to go check some things, b...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>smrorke why are you never on gmail anymore</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Alex_Jeffreys I'd have loved to have come, ju...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Brrrr ! Heading to work.... Chilly today</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>gabriiiiella I neeed to talk to youu..  good ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  no_of_uppercase\n",
              "0  Obama forges his Muslim alliance against the c...                0\n",
              "1  Had the most spectacular prom ever  but now my...                0\n",
              "2  I am overwhelmed today  taking a moment to eat...                1\n",
              "3   lindork Tres sad. I was totally a Max fan.   ...                2\n",
              "4  Crap, I was counting down the hours until my d...                1\n",
              "5   DCBTV  DCBTV I had to go check some things, b...                3\n",
              "6        smrorke why are you never on gmail anymore                 0\n",
              "7   Alex_Jeffreys I'd have loved to have come, ju...                0\n",
              "8          Brrrr ! Heading to work.... Chilly today                 0\n",
              "9   gabriiiiella I neeed to talk to youu..  good ...                1"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Text Cleaning: Conversion to lowercase\n",
        "# ---\n",
        "# Count number of text in uppercase\n",
        "df['no_of_uppercase'] = df.text.apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
        "df[['text','no_of_uppercase']].head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>obama forges his muslim alliance against the c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>had the most spectacular prom ever but now my ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i am overwhelmed today taking a moment to eat ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>lindork tres sad. i was totally a max fan. sytycd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>crap, i was counting down the hours until my d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>dcbtv dcbtv i had to go check some things, buy...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>smrorke why are you never on gmail anymore</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>alex_jeffreys i'd have loved to have come, jus...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>brrrr ! heading to work.... chilly today</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>gabriiiiella i neeed to talk to youu.. good ne...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0  obama forges his muslim alliance against the c...\n",
              "1  had the most spectacular prom ever but now my ...\n",
              "2  i am overwhelmed today taking a moment to eat ...\n",
              "3  lindork tres sad. i was totally a max fan. sytycd\n",
              "4  crap, i was counting down the hours until my d...\n",
              "5  dcbtv dcbtv i had to go check some things, buy...\n",
              "6         smrorke why are you never on gmail anymore\n",
              "7  alex_jeffreys i'd have loved to have come, jus...\n",
              "8           brrrr ! heading to work.... chilly today\n",
              "9  gabriiiiella i neeed to talk to youu.. good ne..."
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# Lowercasing Text\n",
        "# ---\n",
        "df['text'] = df.text.apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "df[['text']].head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "MKO79Jh_wWD_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: wordninja in /usr/local/lib/python3.9/site-packages (2.0.0)\n",
            "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
            "You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: textblob in /usr/local/lib/python3.9/site-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.9/site-packages (from textblob) (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/site-packages (from nltk>=3.1->textblob) (4.64.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/site-packages (from nltk>=3.1->textblob) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/site-packages (from nltk>=3.1->textblob) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/site-packages (from nltk>=3.1->textblob) (8.1.3)\n",
            "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
            "You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Text Cleaning: Splitting concatenated words\n",
        "# ---\n",
        "# Performing this step will take few minutes...\n",
        "# ---\n",
        "\n",
        "\n",
        "# Installing wordnija and textblob\n",
        "# ---\n",
        "#\n",
        "\n",
        "!pip3 install wordninja\n",
        "!pip3 install textblob\n",
        "\n",
        "\n",
        "# Importing those libraries\n",
        "# ---\n",
        "#\n",
        "import wordninja \n",
        "from textblob import TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "xv56Jsl6z_Ew"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5783</th>\n",
              "      <td>tom mcfly i wish i'd be there with you guys</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8719</th>\n",
              "      <td>paul k ne bel boo my jam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3003</th>\n",
              "      <td>dom stark i'd forgotten about that i'm not wor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3057</th>\n",
              "      <td>lar on james get meh wet like haw t sex i moan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2712</th>\n",
              "      <td>n azzi e 86 sorry i haven't exactly had a lot ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8734</th>\n",
              "      <td>tired dd work til 8 then a lovely 4 hour drive...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1034</th>\n",
              "      <td>is going to miss the team when they leave tomo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3060</th>\n",
              "      <td>i wanna go para sailing and jet skiing in bora...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1408</th>\n",
              "      <td>had her ipod playing so loud that her ears hur...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2790</th>\n",
              "      <td>mort ici a 626 we're about to board the plane ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text\n",
              "5783        tom mcfly i wish i'd be there with you guys\n",
              "8719                           paul k ne bel boo my jam\n",
              "3003  dom stark i'd forgotten about that i'm not wor...\n",
              "3057  lar on james get meh wet like haw t sex i moan...\n",
              "2712  n azzi e 86 sorry i haven't exactly had a lot ...\n",
              "8734  tired dd work til 8 then a lovely 4 hour drive...\n",
              "1034  is going to miss the team when they leave tomo...\n",
              "3060  i wanna go para sailing and jet skiing in bora...\n",
              "1408  had her ipod playing so loud that her ears hur...\n",
              "2790  mort ici a 626 we're about to board the plane ..."
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Performing the split\n",
        "# ---\n",
        "#\n",
        "df['text'] = df.text.apply(lambda x: wordninja.split(str(TextBlob(x))))  \n",
        "df['text'] = df.text.str.join(' ')\n",
        "df[['text']].sample(10) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "edYEOU8Tc6sJ"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>punctuation_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>obama forges his muslim alliance against the c...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>had the most spectacular prom ever but now my ...</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i am overwhelmed today taking a moment to eat ...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>lin dork tres sad i was totally a max fan sytycd</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>crap i was counting down the hours until my da...</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>dc b tv dc b tv i had to go check some things ...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>s mr or ke why are you never on gmail anymore</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>alex jeffrey s i'd have loved to have come jus...</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>br rrr heading to work chilly today</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>ga bri iii ella i nee ed to talk to you u good...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  punctuation_count\n",
              "0  obama forges his muslim alliance against the c...                  9\n",
              "1  had the most spectacular prom ever but now my ...                 14\n",
              "2  i am overwhelmed today taking a moment to eat ...                  9\n",
              "3   lin dork tres sad i was totally a max fan sytycd                  8\n",
              "4  crap i was counting down the hours until my da...                  8\n",
              "5  dc b tv dc b tv i had to go check some things ...                  6\n",
              "6      s mr or ke why are you never on gmail anymore                  1\n",
              "7  alex jeffrey s i'd have loved to have come jus...                 12\n",
              "8                br rrr heading to work chilly today                  0\n",
              "9  ga bri iii ella i nee ed to talk to you u good...                  7"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Text Cleaning: Removing punctuation characters\n",
        "# ---\n",
        "# Count number of punctuation chars\n",
        "df['punctuation_count'] = df.text.apply(lambda x: len(\"\".join(_ for _ in x if _ in x.split()))) \n",
        "df[['text', 'punctuation_count']].head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/yy/bnrc1d191h71x84crsy6p5dh0000gn/T/ipykernel_15485/1485397501.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['text'] = df.text.str.replace('[^\\w\\s]','')\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>obama forges his muslim alliance against the c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>had the most spectacular prom ever but now my ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i am overwhelmed today taking a moment to eat ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>lin dork tres sad i was totally a max fan sytycd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>crap i was counting down the hours until my da...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>dc b tv dc b tv i had to go check some things ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>s mr or ke why are you never on gmail anymore</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>alex jeffrey s id have loved to have come just...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>br rrr heading to work chilly today</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>ga bri iii ella i nee ed to talk to you u good...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0  obama forges his muslim alliance against the c...\n",
              "1  had the most spectacular prom ever but now my ...\n",
              "2  i am overwhelmed today taking a moment to eat ...\n",
              "3   lin dork tres sad i was totally a max fan sytycd\n",
              "4  crap i was counting down the hours until my da...\n",
              "5  dc b tv dc b tv i had to go check some things ...\n",
              "6      s mr or ke why are you never on gmail anymore\n",
              "7  alex jeffrey s id have loved to have come just...\n",
              "8                br rrr heading to work chilly today\n",
              "9  ga bri iii ella i nee ed to talk to you u good..."
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Removing Punctuation Characters\n",
        "# ---\n",
        "# \n",
        "df['text'] = df.text.str.replace('[^\\w\\s]','')\n",
        "df[['text']].head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "UxVb9-dJ9zN5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/Barayne/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Text Cleaning: Removing stop words\n",
        "# ---\n",
        "# Import natural language tooklit (nltk) library\n",
        "# \n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "# import a list of stopwords \n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "# View the stopwords\n",
        "stop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>no_of_stopwords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>obama forges his muslim alliance against the c...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>had the most spectacular prom ever but now my ...</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i am overwhelmed today taking a moment to eat ...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>lin dork tres sad i was totally a max fan sytycd</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>crap i was counting down the hours until my da...</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>dc b tv dc b tv i had to go check some things ...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>s mr or ke why are you never on gmail anymore</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>alex jeffrey s id have loved to have come just...</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>br rrr heading to work chilly today</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>ga bri iii ella i nee ed to talk to you u good...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  no_of_stopwords\n",
              "0  obama forges his muslim alliance against the c...                9\n",
              "1  had the most spectacular prom ever but now my ...               13\n",
              "2  i am overwhelmed today taking a moment to eat ...                5\n",
              "3   lin dork tres sad i was totally a max fan sytycd                3\n",
              "4  crap i was counting down the hours until my da...               14\n",
              "5  dc b tv dc b tv i had to go check some things ...                7\n",
              "6      s mr or ke why are you never on gmail anymore                6\n",
              "7  alex jeffrey s id have loved to have come just...               10\n",
              "8                br rrr heading to work chilly today                1\n",
              "9  ga bri iii ella i nee ed to talk to you u good...                4"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Finding Stop Words\n",
        "df['no_of_stopwords'] = df.text.apply(lambda x: len([x for x in x.split() if x in stop]))\n",
        "df[['text','no_of_stopwords']].head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>obama forges muslim alliance civilized world d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>spectacular prom ever bed serenading must answ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>overwhelmed today taking moment eat pray</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>lin dork tres sad totally max fan sytycd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>crap counting hours dad could come home amp he...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>dc b tv dc b tv go check things buy others loo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>mr ke never gmail anymore</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>alex jeffrey id loved come couple unfortunate ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>br rrr heading work chilly today</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>ga bri iii ella nee ed talk u good new sss</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0  obama forges muslim alliance civilized world d...\n",
              "1  spectacular prom ever bed serenading must answ...\n",
              "2           overwhelmed today taking moment eat pray\n",
              "3           lin dork tres sad totally max fan sytycd\n",
              "4  crap counting hours dad could come home amp he...\n",
              "5  dc b tv dc b tv go check things buy others loo...\n",
              "6                          mr ke never gmail anymore\n",
              "7  alex jeffrey id loved come couple unfortunate ...\n",
              "8                   br rrr heading work chilly today\n",
              "9         ga bri iii ella nee ed talk u good new sss"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Remove the stopwords\n",
        "df['text'] = df.text.apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "df[['text']].head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>no_of_stopwords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>obama forges muslim alliance civilized world d...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>spectacular prom ever bed serenading must answ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>overwhelmed today taking moment eat pray</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>lin dork tres sad totally max fan sytycd</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>crap counting hours dad could come home amp he...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>dc b tv dc b tv go check things buy others loo...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>mr ke never gmail anymore</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>alex jeffrey id loved come couple unfortunate ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>br rrr heading work chilly today</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>ga bri iii ella nee ed talk u good new sss</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  no_of_stopwords\n",
              "0  obama forges muslim alliance civilized world d...                0\n",
              "1  spectacular prom ever bed serenading must answ...                0\n",
              "2           overwhelmed today taking moment eat pray                0\n",
              "3           lin dork tres sad totally max fan sytycd                0\n",
              "4  crap counting hours dad could come home amp he...                0\n",
              "5  dc b tv dc b tv go check things buy others loo...                0\n",
              "6                          mr ke never gmail anymore                0\n",
              "7  alex jeffrey id loved come couple unfortunate ...                0\n",
              "8                   br rrr heading work chilly today                0\n",
              "9         ga bri iii ella nee ed talk u good new sss                0"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Conirm stopwords have been removed\n",
        "df['no_of_stopwords'] = df.text.apply(lambda x: len([x for x in x.split() if x in stop]))\n",
        "df[['text','no_of_stopwords']].head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "37RyhNK-q6Df"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /Users/Barayne/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /Users/Barayne/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Text Cleaning: Lemmatization\n",
        "# ---\n",
        "# YOUR CODE GOES BELOW\n",
        "#\n",
        "\n",
        "# For lemmatization, we will need to download wordnet\n",
        "#\n",
        "# import wordnet\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# import 0mw-1.4\n",
        "nltk.download('omw-1.4')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>lemmatization</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>obama forges muslim alliance civilized world d...</td>\n",
              "      <td>obama forge muslim alliance civilized world di...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>spectacular prom ever bed serenading must answ...</td>\n",
              "      <td>spectacular prom ever bed serenading must answ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>overwhelmed today taking moment eat pray</td>\n",
              "      <td>overwhelmed today taking moment eat pray</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>lin dork tres sad totally max fan sytycd</td>\n",
              "      <td>lin dork tres sad totally max fan sytycd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>crap counting hours dad could come home amp he...</td>\n",
              "      <td>crap counting hour dad could come home amp hel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>dc b tv dc b tv go check things buy others loo...</td>\n",
              "      <td>dc b tv dc b tv go check thing buy others look...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>mr ke never gmail anymore</td>\n",
              "      <td>mr ke never gmail anymore</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>alex jeffrey id loved come couple unfortunate ...</td>\n",
              "      <td>alex jeffrey id loved come couple unfortunate ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>br rrr heading work chilly today</td>\n",
              "      <td>br rrr heading work chilly today</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>ga bri iii ella nee ed talk u good new sss</td>\n",
              "      <td>ga bri iii ella nee ed talk u good new ss</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  \\\n",
              "0  obama forges muslim alliance civilized world d...   \n",
              "1  spectacular prom ever bed serenading must answ...   \n",
              "2           overwhelmed today taking moment eat pray   \n",
              "3           lin dork tres sad totally max fan sytycd   \n",
              "4  crap counting hours dad could come home amp he...   \n",
              "5  dc b tv dc b tv go check things buy others loo...   \n",
              "6                          mr ke never gmail anymore   \n",
              "7  alex jeffrey id loved come couple unfortunate ...   \n",
              "8                   br rrr heading work chilly today   \n",
              "9         ga bri iii ella nee ed talk u good new sss   \n",
              "\n",
              "                                       lemmatization  \n",
              "0  obama forge muslim alliance civilized world di...  \n",
              "1  spectacular prom ever bed serenading must answ...  \n",
              "2           overwhelmed today taking moment eat pray  \n",
              "3           lin dork tres sad totally max fan sytycd  \n",
              "4  crap counting hour dad could come home amp hel...  \n",
              "5  dc b tv dc b tv go check thing buy others look...  \n",
              "6                          mr ke never gmail anymore  \n",
              "7  alex jeffrey id loved come couple unfortunate ...  \n",
              "8                   br rrr heading work chilly today  \n",
              "9          ga bri iii ella nee ed talk u good new ss  "
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Lemmatizing our text\n",
        "# ---\n",
        "df['lemmatization'] = df.text.apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()])) \n",
        "df[['text', 'lemmatization']].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-t8mVoSYIAAy"
      },
      "source": [
        "We won't remove numerics because we could loose meaning of our text if we lost the numerics. We could also further prepare our text by performing spelling correction but this is a resource intensive process that we will skip for now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve5fMuCicKkt"
      },
      "source": [
        "#### Feature Engineering Techniques "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "yFwaX_bucVar"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>length_of_tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4812</th>\n",
              "      <td>dev yn burton far away hey im glad made even b...</td>\n",
              "      <td>72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2527</th>\n",
              "      <td>mandy 3000</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9991</th>\n",
              "      <td>gona watch true blood starting</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5236</th>\n",
              "      <td>ryan seacrest rip adam</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2714</th>\n",
              "      <td>michelle mistake get mask</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9010</th>\n",
              "      <td>got home caroline wedding</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>famous pen name lost please help find good home</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4787</th>\n",
              "      <td>loves chocolate milk gf yeah</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>ive got wine laptop dvr full quo worlds dumbes...</td>\n",
              "      <td>63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2572</th>\n",
              "      <td>gods gift 2 even im sleep im work n quo beauty...</td>\n",
              "      <td>56</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text  length_of_tweet\n",
              "4812  dev yn burton far away hey im glad made even b...               72\n",
              "2527                                         mandy 3000               10\n",
              "9991                     gona watch true blood starting               30\n",
              "5236                             ryan seacrest rip adam               22\n",
              "2714                          michelle mistake get mask               25\n",
              "9010                          got home caroline wedding               25\n",
              "76      famous pen name lost please help find good home               47\n",
              "4787                       loves chocolate milk gf yeah               28\n",
              "172   ive got wine laptop dvr full quo worlds dumbes...               63\n",
              "2572  gods gift 2 even im sleep im work n quo beauty...               56"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Feature Construction: Length of tweet\n",
        "# ---\n",
        "df['length_of_tweet'] = df.text.str.len()\n",
        "df[['text','length_of_tweet']].sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WE70M5mYceto"
      },
      "outputs": [],
      "source": [
        "# Feature Construction: Word count \n",
        "# ---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "4yjOYYC4cfji"
      },
      "outputs": [
        {
          "ename": "ZeroDivisionError",
          "evalue": "division by zero",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/Barayne/Documents/DigitalAcademy/PythonProjects/Text_Analysis_Project_Wk8_1.ipynb Cell 40'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Barayne/Documents/DigitalAcademy/PythonProjects/Text_Analysis_Project_Wk8_1.ipynb#ch0000032?line=4'>5</a>\u001b[0m   words \u001b[39m=\u001b[39m sentence\u001b[39m.\u001b[39msplit()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Barayne/Documents/DigitalAcademy/PythonProjects/Text_Analysis_Project_Wk8_1.ipynb#ch0000032?line=5'>6</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m (\u001b[39msum\u001b[39m(\u001b[39mlen\u001b[39m(word) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words)\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(words))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/Barayne/Documents/DigitalAcademy/PythonProjects/Text_Analysis_Project_Wk8_1.ipynb#ch0000032?line=7'>8</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mavg_word_length\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49mtext\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: avg_word(x))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Barayne/Documents/DigitalAcademy/PythonProjects/Text_Analysis_Project_Wk8_1.ipynb#ch0000032?line=8'>9</a>\u001b[0m df[[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mavg_word_length\u001b[39m\u001b[39m'\u001b[39m]]\u001b[39m.\u001b[39msample(\u001b[39m5\u001b[39m)\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/core/series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/site-packages/pandas/core/series.py?line=4322'>4323</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/site-packages/pandas/core/series.py?line=4323'>4324</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/site-packages/pandas/core/series.py?line=4324'>4325</a>\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/site-packages/pandas/core/series.py?line=4327'>4328</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/site-packages/pandas/core/series.py?line=4328'>4329</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/site-packages/pandas/core/series.py?line=4329'>4330</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/site-packages/pandas/core/series.py?line=4330'>4331</a>\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/site-packages/pandas/core/series.py?line=4331'>4332</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/site-packages/pandas/core/series.py?line=4430'>4431</a>\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/site-packages/pandas/core/series.py?line=4431'>4432</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///usr/local/lib/python3.9/site-packages/pandas/core/series.py?line=4432'>4433</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/core/apply.py:1082\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/site-packages/pandas/core/apply.py?line=1077'>1078</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mstr\u001b[39m):\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/site-packages/pandas/core/apply.py?line=1078'>1079</a>\u001b[0m     \u001b[39m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/site-packages/pandas/core/apply.py?line=1079'>1080</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m-> <a href='file:///usr/local/lib/python3.9/site-packages/pandas/core/apply.py?line=1081'>1082</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/core/apply.py:1137\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/site-packages/pandas/core/apply.py?line=1130'>1131</a>\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/site-packages/pandas/core/apply.py?line=1131'>1132</a>\u001b[0m         \u001b[39m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/site-packages/pandas/core/apply.py?line=1132'>1133</a>\u001b[0m         \u001b[39m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/site-packages/pandas/core/apply.py?line=1133'>1134</a>\u001b[0m         \u001b[39m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/site-packages/pandas/core/apply.py?line=1134'>1135</a>\u001b[0m         \u001b[39m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/site-packages/pandas/core/apply.py?line=1135'>1136</a>\u001b[0m         \u001b[39m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///usr/local/lib/python3.9/site-packages/pandas/core/apply.py?line=1136'>1137</a>\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/site-packages/pandas/core/apply.py?line=1137'>1138</a>\u001b[0m             values,\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/site-packages/pandas/core/apply.py?line=1138'>1139</a>\u001b[0m             f,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/site-packages/pandas/core/apply.py?line=1139'>1140</a>\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/site-packages/pandas/core/apply.py?line=1140'>1141</a>\u001b[0m         )\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/site-packages/pandas/core/apply.py?line=1142'>1143</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/site-packages/pandas/core/apply.py?line=1143'>1144</a>\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/site-packages/pandas/core/apply.py?line=1144'>1145</a>\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.9/site-packages/pandas/core/apply.py?line=1145'>1146</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
            "File \u001b[0;32mpandas/_libs/lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[1;32m/Users/Barayne/Documents/DigitalAcademy/PythonProjects/Text_Analysis_Project_Wk8_1.ipynb Cell 40'\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Barayne/Documents/DigitalAcademy/PythonProjects/Text_Analysis_Project_Wk8_1.ipynb#ch0000032?line=4'>5</a>\u001b[0m   words \u001b[39m=\u001b[39m sentence\u001b[39m.\u001b[39msplit()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Barayne/Documents/DigitalAcademy/PythonProjects/Text_Analysis_Project_Wk8_1.ipynb#ch0000032?line=5'>6</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m (\u001b[39msum\u001b[39m(\u001b[39mlen\u001b[39m(word) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words)\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(words))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/Barayne/Documents/DigitalAcademy/PythonProjects/Text_Analysis_Project_Wk8_1.ipynb#ch0000032?line=7'>8</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mavg_word_length\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: avg_word(x))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Barayne/Documents/DigitalAcademy/PythonProjects/Text_Analysis_Project_Wk8_1.ipynb#ch0000032?line=8'>9</a>\u001b[0m df[[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mavg_word_length\u001b[39m\u001b[39m'\u001b[39m]]\u001b[39m.\u001b[39msample(\u001b[39m5\u001b[39m)\n",
            "\u001b[1;32m/Users/Barayne/Documents/DigitalAcademy/PythonProjects/Text_Analysis_Project_Wk8_1.ipynb Cell 40'\u001b[0m in \u001b[0;36mavg_word\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Barayne/Documents/DigitalAcademy/PythonProjects/Text_Analysis_Project_Wk8_1.ipynb#ch0000032?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mavg_word\u001b[39m(sentence):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Barayne/Documents/DigitalAcademy/PythonProjects/Text_Analysis_Project_Wk8_1.ipynb#ch0000032?line=4'>5</a>\u001b[0m   words \u001b[39m=\u001b[39m sentence\u001b[39m.\u001b[39msplit()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/Barayne/Documents/DigitalAcademy/PythonProjects/Text_Analysis_Project_Wk8_1.ipynb#ch0000032?line=5'>6</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m (\u001b[39msum\u001b[39;49m(\u001b[39mlen\u001b[39;49m(word) \u001b[39mfor\u001b[39;49;00m word \u001b[39min\u001b[39;49;00m words)\u001b[39m/\u001b[39;49m\u001b[39mlen\u001b[39;49m(words))\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ],
      "source": [
        "# Feature Construction: Word density (Average no. of words / tweet)\n",
        "# ---\n",
        "\n",
        "def avg_word(sentence):\n",
        "  words = sentence.split()\n",
        "  return (sum(len(word) for word in words)/len(words))\n",
        "\n",
        "df['avg_word_length'] = df.text.apply(lambda x: avg_word(x))\n",
        "df[['text','avg_word_length']].sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "aoSoEEctcgRq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/Barayne/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /Users/Barayne/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Feature Construction: Noun count\n",
        "# ---\n",
        "# YOUR CODE GOES BELOW\n",
        "#\n",
        "# First, we will download the punkt and the averaged_perceptron_tagger into our notebook environment. \n",
        "# which will allow us to find the part of speech tags.\n",
        "# ---\n",
        "#\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We create the function to check and get the part of speech tag count of a words in a given sentence\n",
        "pos_dic = {\n",
        "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
        "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
        "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
        "    'adj' :  ['JJ','JJR','JJS'],\n",
        "    'adv' : ['RB','RBR','RBS','WRB']\n",
        "}\n",
        "\n",
        "def pos_check(x, flag):\n",
        "    cnt = 0\n",
        "    try:\n",
        "        wiki = TextBlob(x)\n",
        "        for tup in wiki.tags:\n",
        "            ppo = list(tup)[1]\n",
        "            if ppo in pos_dic[flag]:\n",
        "                cnt += 1\n",
        "    except:\n",
        "        pass\n",
        "    return cnt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "yxzVY6q60V1X"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>noun_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8881</th>\n",
              "      <td>finishing talking everything fine strange thin...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9318</th>\n",
              "      <td>summer kick suite freedom tonight eh still sha...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5559</th>\n",
              "      <td>dougie mcfly mcfly day brazil special mcfly ho...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8000</th>\n",
              "      <td>kenner jacobs indeed coming see u b 4 u left w...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4626</th>\n",
              "      <td>kev b due nt sad</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3137</th>\n",
              "      <td>blog hopping ill never get tired everyday</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5077</th>\n",
              "      <td>time warner peeps without power right</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2999</th>\n",
              "      <td>upset kicked peyton lucas one tree hill mik call</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8730</th>\n",
              "      <td>moving process officially begins new apt sweet</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1087</th>\n",
              "      <td>toyota gladstone try rub spent afternoon pools...</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text  noun_count\n",
              "8881  finishing talking everything fine strange thin...           6\n",
              "9318  summer kick suite freedom tonight eh still sha...           7\n",
              "5559  dougie mcfly mcfly day brazil special mcfly ho...           5\n",
              "8000  kenner jacobs indeed coming see u b 4 u left w...           6\n",
              "4626                                   kev b due nt sad           2\n",
              "3137          blog hopping ill never get tired everyday           2\n",
              "5077              time warner peeps without power right           5\n",
              "2999   upset kicked peyton lucas one tree hill mik call           3\n",
              "8730     moving process officially begins new apt sweet           2\n",
              "1087  toyota gladstone try rub spent afternoon pools...          12"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Noun Count\n",
        "# ---\n",
        "df['noun_count'] = df.text.apply(lambda x: pos_check(x, 'noun'))\n",
        "df[['text','noun_count']].sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "Y0hHkQb_cfNI"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>verb_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>322</th>\n",
              "      <td>ri krak shop thanks much</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6280</th>\n",
              "      <td>kfar 10 wu h wu h wu h lauren lol</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5151</th>\n",
              "      <td>go lee cy go lee cy yes girl l im dead serious</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7815</th>\n",
              "      <td>im bus</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4502</th>\n",
              "      <td>sun scrape bai im paris anymore na ko tet bai</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4743</th>\n",
              "      <td>zaibatsu arent looking</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7030</th>\n",
              "      <td>need get together</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1990</th>\n",
              "      <td>google app engine doesnt support j axb rest ea...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9507</th>\n",
              "      <td>lance armstrong agree please show</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6161</th>\n",
              "      <td>tom mcfly went hilton three times stayed day u...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text  verb_count\n",
              "322                            ri krak shop thanks much           0\n",
              "6280                  kfar 10 wu h wu h wu h lauren lol           1\n",
              "5151     go lee cy go lee cy yes girl l im dead serious           4\n",
              "7815                                             im bus           0\n",
              "4502      sun scrape bai im paris anymore na ko tet bai           0\n",
              "4743                             zaibatsu arent looking           1\n",
              "7030                                  need get together           1\n",
              "1990  google app engine doesnt support j axb rest ea...           1\n",
              "9507                  lance armstrong agree please show           0\n",
              "6161  tom mcfly went hilton three times stayed day u...           1"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Feature Construction: Verb count\n",
        "# ---\n",
        "df['verb_count'] = df.text.apply(lambda x: pos_check(x, 'verb'))\n",
        "df[['text','verb_count']].sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "7foa2jELcdc7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>adj_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9911</th>\n",
              "      <td>bobby l lew lister k ry ten live one night</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4688</th>\n",
              "      <td>arm deacon night</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2560</th>\n",
              "      <td>morning im soo glad sunny today makes smile tw...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4595</th>\n",
              "      <td>rubber pacers suck 6 mouth pain eat anything</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8009</th>\n",
              "      <td>miss clar cal</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1156</th>\n",
              "      <td>laura walker xo sister hah beck nelson going g</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1660</th>\n",
              "      <td>eh</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8521</th>\n",
              "      <td>david archie hi really dont anything say wanna...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6505</th>\n",
              "      <td>sold 1 st car ebay pleasure surprise yay shall</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5912</th>\n",
              "      <td>pitch engine lol charge head heart didnt even ...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text  adj_count\n",
              "9911         bobby l lew lister k ry ten live one night          2\n",
              "4688                                   arm deacon night          0\n",
              "2560  morning im soo glad sunny today makes smile tw...          2\n",
              "4595       rubber pacers suck 6 mouth pain eat anything          0\n",
              "8009                                      miss clar cal          2\n",
              "1156     laura walker xo sister hah beck nelson going g          0\n",
              "1660                                                 eh          0\n",
              "8521  david archie hi really dont anything say wanna...          2\n",
              "6505     sold 1 st car ebay pleasure surprise yay shall          2\n",
              "5912  pitch engine lol charge head heart didnt even ...          3"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Feature Construction: Adjective count / Tweet\n",
        "# ---\n",
        "df['adj_count'] = df.text.apply(lambda x: pos_check(x, 'adj'))\n",
        "df[['text','adj_count']].sample(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "smNLu-KJcdMT"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>adv_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7564</th>\n",
              "      <td>im sjsu studying reg holy cow reg looks tough</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5646</th>\n",
              "      <td>laundry done ironing packing</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>going vegas maybe maybe ill two months</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>701</th>\n",
              "      <td>fat man johnson atl hah needs cop younger sister</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2329</th>\n",
              "      <td>wanna take sun bath backyard go dentist well m...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8281</th>\n",
              "      <td>tomorrow tomorrow love ya tomorrow remember th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8012</th>\n",
              "      <td>bore eeeeee e ed</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1399</th>\n",
              "      <td>lovin new blackberry</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5114</th>\n",
              "      <td>beautiful day airsoft</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5635</th>\n",
              "      <td>foto lex ic sadly thats whats happening good</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text  adv_count\n",
              "7564      im sjsu studying reg holy cow reg looks tough          0\n",
              "5646                       laundry done ironing packing          0\n",
              "80               going vegas maybe maybe ill two months          2\n",
              "701    fat man johnson atl hah needs cop younger sister          0\n",
              "2329  wanna take sun bath backyard go dentist well m...          3\n",
              "8281  tomorrow tomorrow love ya tomorrow remember th...          0\n",
              "8012                                   bore eeeeee e ed          1\n",
              "1399                               lovin new blackberry          0\n",
              "5114                              beautiful day airsoft          0\n",
              "5635       foto lex ic sadly thats whats happening good          1"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Feature Construction: Adverb count / Tweet\n",
        "# ---\n",
        "df['adv_count'] = df.text.apply(lambda x: pos_check(x, 'adv'))\n",
        "df[['text','adv_count']].sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "tRjeP3R2dDWA"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>pron_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9159</th>\n",
              "      <td>ugh bronchitis horrible going bed</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1772</th>\n",
              "      <td>stretch able pants butt uncomfortable</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8724</th>\n",
              "      <td>first online purchase im flickr pro</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3672</th>\n",
              "      <td>ram las thx follow friday recommendation</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>obama forges muslim alliance civilized world d...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7707</th>\n",
              "      <td>finished treasury lots colors back work</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>770</th>\n",
              "      <td>go banana tailpipe said hell stick ports</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>unn allman yeah looks like quo busy quo fuckin...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>709</th>\n",
              "      <td>neither sol j el job looks like problem dentist</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>sore throat runny nose lethargy fun</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text  pron_count\n",
              "9159                  ugh bronchitis horrible going bed           0\n",
              "1772              stretch able pants butt uncomfortable           0\n",
              "8724                first online purchase im flickr pro           0\n",
              "3672           ram las thx follow friday recommendation           0\n",
              "0     obama forges muslim alliance civilized world d...           0\n",
              "7707            finished treasury lots colors back work           0\n",
              "770            go banana tailpipe said hell stick ports           0\n",
              "79    unn allman yeah looks like quo busy quo fuckin...           1\n",
              "709     neither sol j el job looks like problem dentist           0\n",
              "69                  sore throat runny nose lethargy fun           0"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Feature Construction: Pronoun \n",
        "# ---\n",
        "df['pron_count'] = df.text.apply(lambda x: pos_check(x, 'pron'))\n",
        "df[['text','pron_count']].sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "r1ab-9bOdEAT"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>subjectivity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9588</th>\n",
              "      <td>jem maha tty really hope joke j emma today fir...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9334</th>\n",
              "      <td>kids australia green tea designs thanks girls ...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9855</th>\n",
              "      <td>telecom bum im showing youve online consistent...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2385</th>\n",
              "      <td>x r sorry bad night</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1693</th>\n",
              "      <td>mr jack enjoy rush work last series unfortunat...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7490</th>\n",
              "      <td>kelly dot compton going oh thats everyone skin...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8681</th>\n",
              "      <td>thats good sleep shouldnt started morning eati...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7880</th>\n",
              "      <td>dd lovato words sad buu u</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1028</th>\n",
              "      <td>got home chinese food put shay las info blast ...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6787</th>\n",
              "      <td>st b jordan know deadlines multi task theyll hear</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text  subjectivity\n",
              "9588  jem maha tty really hope joke j emma today fir...           0.0\n",
              "9334  kids australia green tea designs thanks girls ...           0.0\n",
              "9855  telecom bum im showing youve online consistent...           0.0\n",
              "2385                                x r sorry bad night           0.0\n",
              "1693  mr jack enjoy rush work last series unfortunat...           0.0\n",
              "7490  kelly dot compton going oh thats everyone skin...           0.0\n",
              "8681  thats good sleep shouldnt started morning eati...           0.0\n",
              "7880                          dd lovato words sad buu u           0.0\n",
              "1028  got home chinese food put shay las info blast ...           0.0\n",
              "6787  st b jordan know deadlines multi task theyll hear           0.0"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Feature Construction: Subjectivity\n",
        "# ---\n",
        "# YOUR CODE GOES BELOW\n",
        "# \n",
        "def get_subjectivity(text):\n",
        "    try:\n",
        "        textblob = TextBlob(unicode(text, 'utf-8'))\n",
        "        subj = textblob.sentiment.subjectivity\n",
        "    except:\n",
        "        subj = 0.0\n",
        "    return subj\n",
        "\n",
        "df['subjectivity'] = df.text.apply(get_subjectivity)\n",
        "df[['text', 'subjectivity']].sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "rEt889AByeHq"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>536</th>\n",
              "      <td>terrence j 106 drunk hell yes laid gimme</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3300</th>\n",
              "      <td>princess 2802 hah miss boy smells matter got a...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9793</th>\n",
              "      <td>n takaya could much fun testing different play...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>477</th>\n",
              "      <td>hey tear catcher theyve belgium shows</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2761</th>\n",
              "      <td>listen please anyone</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9628</th>\n",
              "      <td>bl ka demi c eh en fo di ou ke gen yon le glis...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9296</th>\n",
              "      <td>baz anna mother laughing lol bl c gr lt 3 ssc</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3150</th>\n",
              "      <td>watching nutty madams videos</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020</th>\n",
              "      <td>twitter titter started nothing still</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1604</th>\n",
              "      <td>sad beautiful get local recommendations change...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text  polarity\n",
              "536            terrence j 106 drunk hell yes laid gimme       0.0\n",
              "3300  princess 2802 hah miss boy smells matter got a...       0.0\n",
              "9793  n takaya could much fun testing different play...       0.0\n",
              "477               hey tear catcher theyve belgium shows       0.0\n",
              "2761                               listen please anyone       0.0\n",
              "9628  bl ka demi c eh en fo di ou ke gen yon le glis...       0.0\n",
              "9296      baz anna mother laughing lol bl c gr lt 3 ssc       0.0\n",
              "3150                       watching nutty madams videos       0.0\n",
              "2020               twitter titter started nothing still       0.0\n",
              "1604  sad beautiful get local recommendations change...       0.0"
            ]
          },
          "execution_count": 130,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Feature Construction: Polarity\n",
        "# ---\n",
        "def get_polarity(text):\n",
        "    try:\n",
        "        textblob = TextBlob(unicode(text, 'utf-8'))\n",
        "        pol = textblob.sentiment.polarity\n",
        "    except:\n",
        "        pol = 0.0\n",
        "    return pol\n",
        "\n",
        "df['polarity'] = df.text.apply(get_polarity)\n",
        "df[['text', 'polarity']].sample(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Library for TD-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "geXOqgRLdCBL"
      },
      "outputs": [],
      "source": [
        "# Feature Construction: Word Level N-Gram TF-IDF Feature \n",
        "# ---\n",
        "\n",
        "# from nltk import word_tokenize, ngrams\n",
        "\n",
        "# list(ngrams(word_tokenize(df['text'][0]), 2)) \n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word', ngram_range=(1,3),  stop_words= 'english')\n",
        "df_word_vect = tfidf.fit_transform(df.text) \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "C77ntLfXdTAa"
      },
      "outputs": [],
      "source": [
        "# Feature Construction: Character Level N-Gram TF-IDF Feature\n",
        "# ---\n",
        "# list(ngrams(df['text'][0], 2))\n",
        "tfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='char', ngram_range=(1,3),  stop_words= 'english')\n",
        "df_char_vect = tfidf.fit_transform(df.text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "75PUieNPl6j5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0, 9,\n",
              "        'obama forges his muslim alliance against the civilized world and he didnt even drop in for a cup of tea',\n",
              "        ..., 3, 1, 1],\n",
              "       [0, 14,\n",
              "        'had the most spectacular prom ever but now my bed is serenading me and i must answer sweet dreams my friends what a wonderful day',\n",
              "        ..., 3, 2, 1],\n",
              "       [1, 9, 'i am overwhelmed today taking a moment to eat and pray',\n",
              "        ..., 2, 0, 0],\n",
              "       ...,\n",
              "       [0, 5, 'hah a linas hyper already well lucky you im in college',\n",
              "        ..., 1, 2, 2],\n",
              "       [0, 0, 'omg really good day happened right here', ..., 1, 2, 1],\n",
              "       [0, 7,\n",
              "        'love 2 cook pie i saw you on division and 68 th but you didnt see me',\n",
              "        ..., 3, 0, 0]], dtype=object)"
            ]
          },
          "execution_count": 134,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's prepare the constructed features for modeling\n",
        "# ---\n",
        "#\n",
        "X_metadata = np.array(df.iloc[:, 2:12])\n",
        "X_metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "Wn_kBCljx6PS"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "no supported conversion for types: (dtype('float64'), dtype('float64'), dtype('O'))",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/Users/Barayne/Documents/DigitalAcademy/PythonProjects/Text_Analysis_Project_Wk8_1.ipynb Cell 54'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Barayne/Documents/DigitalAcademy/PythonProjects/Text_Analysis_Project_Wk8_1.ipynb#ch0000044?line=0'>1</a>\u001b[0m \u001b[39m# We combine our two tfidf (sparse) matrices and X_metadata\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Barayne/Documents/DigitalAcademy/PythonProjects/Text_Analysis_Project_Wk8_1.ipynb#ch0000044?line=1'>2</a>\u001b[0m \u001b[39m# ---\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Barayne/Documents/DigitalAcademy/PythonProjects/Text_Analysis_Project_Wk8_1.ipynb#ch0000044?line=2'>3</a>\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/Barayne/Documents/DigitalAcademy/PythonProjects/Text_Analysis_Project_Wk8_1.ipynb#ch0000044?line=3'>4</a>\u001b[0m X \u001b[39m=\u001b[39m scipy\u001b[39m.\u001b[39;49msparse\u001b[39m.\u001b[39;49mhstack([df_word_vect, df_char_vect,  X_metadata])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Barayne/Documents/DigitalAcademy/PythonProjects/Text_Analysis_Project_Wk8_1.ipynb#ch0000044?line=4'>5</a>\u001b[0m X\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/scipy/sparse/_construct.py:519\u001b[0m, in \u001b[0;36mhstack\u001b[0;34m(blocks, format, dtype)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/scipy/sparse/_construct.py?line=488'>489</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhstack\u001b[39m(blocks, \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/scipy/sparse/_construct.py?line=489'>490</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/scipy/sparse/_construct.py?line=490'>491</a>\u001b[0m \u001b[39m    Stack sparse matrices horizontally (column wise)\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/scipy/sparse/_construct.py?line=491'>492</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/scipy/sparse/_construct.py?line=516'>517</a>\u001b[0m \n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/scipy/sparse/_construct.py?line=517'>518</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.9/site-packages/scipy/sparse/_construct.py?line=518'>519</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m bmat([blocks], \u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mformat\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mdtype)\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/scipy/sparse/_construct.py:666\u001b[0m, in \u001b[0;36mbmat\u001b[0;34m(blocks, format, dtype)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/scipy/sparse/_construct.py?line=663'>664</a>\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/scipy/sparse/_construct.py?line=664'>665</a>\u001b[0m     all_dtypes \u001b[39m=\u001b[39m [blk\u001b[39m.\u001b[39mdtype \u001b[39mfor\u001b[39;00m blk \u001b[39min\u001b[39;00m blocks[block_mask]]\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.9/site-packages/scipy/sparse/_construct.py?line=665'>666</a>\u001b[0m     dtype \u001b[39m=\u001b[39m upcast(\u001b[39m*\u001b[39;49mall_dtypes) \u001b[39mif\u001b[39;00m all_dtypes \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/scipy/sparse/_construct.py?line=667'>668</a>\u001b[0m row_offsets \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mappend(\u001b[39m0\u001b[39m, np\u001b[39m.\u001b[39mcumsum(brow_lengths))\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/scipy/sparse/_construct.py?line=668'>669</a>\u001b[0m col_offsets \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mappend(\u001b[39m0\u001b[39m, np\u001b[39m.\u001b[39mcumsum(bcol_lengths))\n",
            "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/scipy/sparse/_sputils.py:51\u001b[0m, in \u001b[0;36mupcast\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.9/site-packages/scipy/sparse/_sputils.py?line=47'>48</a>\u001b[0m         _upcast_memo[\u001b[39mhash\u001b[39m(args)] \u001b[39m=\u001b[39m t\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.9/site-packages/scipy/sparse/_sputils.py?line=48'>49</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m t\n\u001b[0;32m---> <a href='file:///usr/local/lib/python3.9/site-packages/scipy/sparse/_sputils.py?line=50'>51</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mno supported conversion for types: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (args,))\n",
            "\u001b[0;31mTypeError\u001b[0m: no supported conversion for types: (dtype('float64'), dtype('float64'), dtype('O'))"
          ]
        }
      ],
      "source": [
        "# We combine our two tfidf (sparse) matrices and X_metadata\n",
        "# ---\n",
        "#\n",
        "X = scipy.sparse.hstack([df_word_vect, df_char_vect,  X_metadata])\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "mL7etsSY8cfy"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 4, 0, ..., 0, 4, 0])"
            ]
          },
          "execution_count": 109,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Getting our response variable\n",
        "# ---\n",
        "#\n",
        "y = np.array(df.iloc[:, 0])\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_iOhAPnaERN"
      },
      "source": [
        "### 4. Data Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BKyd7Uwl-Vr"
      },
      "source": [
        "During this step, we will use machine learning algorithms to train and test our sentiment analysis models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "533B2cK_Ey3Z"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'X' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/Users/Barayne/Documents/DigitalAcademy/PythonProjects/Text_Analysis_Project_Wk8_1.ipynb Cell 57'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Barayne/Documents/DigitalAcademy/PythonProjects/Text_Analysis_Project_Wk8_1.ipynb#ch0000048?line=0'>1</a>\u001b[0m \u001b[39m# Splitting our data\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Barayne/Documents/DigitalAcademy/PythonProjects/Text_Analysis_Project_Wk8_1.ipynb#ch0000048?line=1'>2</a>\u001b[0m \u001b[39m# ---\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Barayne/Documents/DigitalAcademy/PythonProjects/Text_Analysis_Project_Wk8_1.ipynb#ch0000048?line=2'>3</a>\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Barayne/Documents/DigitalAcademy/PythonProjects/Text_Analysis_Project_Wk8_1.ipynb#ch0000048?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/Barayne/Documents/DigitalAcademy/PythonProjects/Text_Analysis_Project_Wk8_1.ipynb#ch0000048?line=4'>5</a>\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X, y, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ],
      "source": [
        "# Splitting our data\n",
        "# ---\n",
        "#\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zx3rCuu6ddht"
      },
      "outputs": [],
      "source": [
        "# Fitting our model\n",
        "# ---\n",
        "#\n",
        "\n",
        "# Importing the algorithms\n",
        "from sklearn.naive_bayes import MultinomialNB \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "nb_classifier = MultinomialNB() \n",
        "lr_classifier = LogisticRegression(max_iter=1000) \n",
        "\n",
        "# Training our model\n",
        "nb_classifier.fit(X_train, y_train) \n",
        "lr_classifier.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grL4WhdTYu5g"
      },
      "outputs": [],
      "source": [
        "# Making predictions\n",
        "# ---\n",
        "#\n",
        "y_predict_nb = nb_classifier.predict(X_test) \n",
        "y_predict_lr = lr_classifier.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyb3cihzdlKX"
      },
      "outputs": [],
      "source": [
        "# Evaluating the Models\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Accuracy scores\n",
        "# ---\n",
        "#\n",
        "print(\"Naive Bayes Classifier:\\n\", accuracy_score(y_test, y_predict_nb)) \n",
        "print(\"Logistic Regression Classifier: \\n\", accuracy_score(y_test, y_predict_lr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obyOpb9uD9IM"
      },
      "outputs": [],
      "source": [
        "# Confusion matrices\n",
        "# ---\n",
        "# \n",
        "print(\"Naive Bayes Classifier: \\n\", confusion_matrix(y_test, y_predict_nb)) \n",
        "print(\"Logistic Regression Classifier: \\n\", confusion_matrix(y_test, y_predict_lr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFdanMG4D6Dn"
      },
      "outputs": [],
      "source": [
        "# Classification Reports\n",
        "# ---\n",
        "#\n",
        "print(\"Naive Bayes Classifier: \\n\", classification_report(y_test, y_predict_nb)) \n",
        "print(\"Logistic Regression Classifier: \\n\", classification_report(y_test, y_predict_lr))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxETCaYyOyHv"
      },
      "source": [
        "**Evaluation our Models**\n",
        "\n",
        "* **Accuracy:** the percentage of texts that were assigned the correct topic.\n",
        "* **Precision:** the percentage of texts the classifier classified correctly out of the total number of texts it predicted for each topic\n",
        "* **Recall:** the percentage of texts the model predicted for each topic out of the total number of texts it should have predicted for that topic.\n",
        "* **F1 Score:** the average of both precision and recall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbisAqRswA80"
      },
      "source": [
        "To improve our model, we can try perfoming other text processing techniques that would better prepare our data for fitting our model. We can also use different vectorizing techniques, implement other machine learning models and perform hyperparameter tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWiaaYPCnsiC"
      },
      "source": [
        "### 5. Recommendations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3DQO1yFnvx3"
      },
      "source": [
        "Our best model had an accuracy of 73.25% and use it for classifying newer tweets. We can improve this performance by performing hyperparameter tuning and feature engineering methods. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "enxDwtXPZTpT",
        "85tygFqJZ0Xw",
        "96Uz3PxJZ6E7",
        "YlHYpKxfHRJ4",
        "LWiaaYPCnsiC"
      ],
      "name": "Text Analysis_Project_Wk8.1",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
